---
title: "Task 3: Multiple Regression in R"
output: html_notebook
---

This notebook contains the answers to Task 3:Multiple Regression in R.


```{r - Initial imports}
library(readr)
library(caret)
library(ggplot2)
library(dplyr)
library(corrplot)

library(gbm)
library(Rcpp)
```

```{r - Read the dataset from memory}
train_df<- read.csv("existingproductattributes2017.csv")#Clients who included their favorite computer brand in the questionaire answers
test_df<- read.csv("newproductattributes2017.csv") #Clients that did not favorite computer brand
train_df
test_df
```

These are the two datasets to be used for this task. 
It's worth noting that the column "volume" in the test_df has been filled with zeroes (as default). Therefore this column is meaningless as it is and should not be used directly.

*Understand the data:*

```{r - Getting to know the training data}
#print("List your attributes within your data set.")
#attributes(train_df)  # Too long
print("Prints the min, max, mean, median, and quartiles of each attribute.")
summary(train_df)
print("Displays the structure of your data set.")
str(train_df)
print("Names your attributes within your data set.")
names(train_df)
#print("Will print out the instances within that particular column in your data set.")
#train_df$salary # Too long
#train_df$age  # Too long
```

```{r - Compare if the testing data is similar to the training data (except for the volume column)}
print("Prints the min, max, mean, median, and quartiles of each attribute.")
summary(test_df)
print("Displays the structure of your data set.")
str(test_df)
print("Names your attributes within your data set.")
names(test_df)
```
The summary for each column is quite similar between these two dataframes. In fact, the minimum and maximum value are exactly the same for all columns except for the "volume" (which is comprehensible). The value for 1st and 3rd quartile as well as the mean and median is also pretty similar.

```{r - Do column types in train dataset coincide with their data? Or do I need to convert data types?}
sapply(train_df, typeof)
sapply(train_df, typeof) == sapply(test_df, typeof)

```
The column types coincide, but the variable ProductType is of character type. Therefore, I will dummify this column into a binary One hot encoding with caret. The remaining columns are of numerical type so no further adjustments needed here. 

```{r - Dummify the data:}
#Convert categorical columns into factors to indicate that these columns contain a limited number of unique values.
length(table(train_df$ProductType))


# Rewrite the train_df with the dummy variables:
newDataFrame <- dummyVars(" ~ .", data = train_df)
train_df <- data.frame(predict(newDataFrame, newdata = train_df))
train_df

```
The column ProductType has 12 possible values.
As we previously had 18 columns in total, after dummifying the data both datasets will contain (18-1(removal of original column)+12=) 29 columns, which is what we have.

Now I will perform the same process on the test dataset so both datasets remain identical and ready to use:

```{r - Replicate same conversions to test_df:}
# Rewrite the train_df with the dummy variables:
newDataFrame2 <- dummyVars(" ~ .", data = test_df)
test_df <- data.frame(predict(newDataFrame2, newdata = test_df))
test_df
```
The same 29 columns are verified here.

```{r - Are there missing values?}
any(is.na(train_df))
any(is.na(test_df))

```
The train_df has null values. Investigate:

```{r - Which columns have null values:}
names(which(colSums(is.na(train_df)) > 0))
train_df$BestSellersRank
```
As can be seen above the column BestSellersRank in the train_df has got quite a few Null values. According to the plan of attack I should just ignore the variables with nulls. (Otherwise, I would address these nulls by replacing them with whatever one should do when correcting ranking variables. Out of the top of my head, I would quite possibly use the median as replacement value. This way, the replaced nulls would be pretty neutral when looking at this variable as a whole)


```{r - Address missing values of both datasets by simply eliminating the column:}
train_df$BestSellersRank <- NULL
train_df

test_df$BestSellersRank <- NULL
test_df
```

*Exploratory Data Analysis*

Following the Plan of Attack, I will now calculate the correlations, but before I will reconfirm that all variables are of numerical type in both datasets:

```{r}
str(train_df)
str(test_df)
```
Yes they are!


```{r - Correlations}
corrData_train <- cor(train_df)
#corrData_train

corrData_test <- cor(test_df)
#corrData_test

```

Instead of printing the matrixes I will simply plot a correlation heat map as it presents the data in a more readable and concise way.

NOTE: Instead of posting the two correlation plots in the R markdown below I saved them as pictures to be seen with higher quality.


```{r - Correlation plot for the train dataset:}
png(file="corr_train.png", res=300, width=4500, height=4500)
corrplot(corrData_train, method ='number')
dev.off() #Avoid overwritting png image above
```

```{r - Correlation plot for the test dataset:}
png(file="corr_test.png", res=300, width=4500, height=4500)
corrplot(corrData_test, method ='number')
dev.off()#Avoid overwritting png image above
```
The correlations from both datasets are quite similar. The biggest difference is that the Volume has no correlation in the test dataframe because this column is invariable (looking at the formula there is a division by zero in this case resulting in an undefined number).

All x'N'StarReviews variables are significantly positively correlated among each other. Interestingly, the further the classification the lower the correlation is. This fact makes sense because these variables are a simple count. I would argue that the high correlation among these features can be owed to their common nature, a causation relation seems to be unlikely among these variables.
The variables associated with product dimensions are also notably positively correlated. Once again because of the nature of these features, it makes sense that these are quite correlated. Packaging dimension and weight follow certain rules.

Other than these, there are a few relevant individual situations mostly of positive correlations such as the case of ProductType Laptop with Price or ProductType Printer with Profit Margin in the testing dataset and Volume with the star reviews (particularly with 5 star review which has a correlation of 1!) or Extended Warranty with Profit margin.

Finally, looking exclusively to the correlations of the volume in the train dataset, it is visible that the products with any StarReview or PositiveServiceReview have a much higher volume (Review associated features are possibily be the most relevant features towards predicting the volume of sales). Another interesting outtake from this correlation matrix is that Game Consoles have a much correlation with volume than any other prduct type, meaning that these have higher volumes.
Variables such as Profit Margin or any of the product dimensions have a correlation closer to zero with Volume. These variables may not be relevant towards predicting this target.

*Creating testing and training sets with caret:*
```{r}

inTrain <- createDataPartition(y = train_df$Volume,
            ## the outcome data are needed
            p = .75,# The percentage of data in the training set should be 75%
            list = FALSE)
#The output is a set of integers for the rows of Sonar that belong in the training set.

training <- train_df[ inTrain,] #TrainSet will contain the selected indices.
testing <- train_df[-inTrain,] #TestSet will contain only the remaining rows that have not been selected.

nrow(training)
nrow(testing)

```

**Caret Model 1: Create a linear model that uses volume as its dependent variable:**

```{r}
#Write trainSet in the data parameter instead of on each variable just to improve readability
LinearModel<- lm(Volume ~ PositiveServiceReview, training) #The analysis goal is to predict the Volume of a product using the most informative singular feature (which appears to be PositveServiceReview).
summary(LinearModel)

predictions <- LinearModel %>% predict(testing)

RMSE(predictions, testing$Volume)
R2(predictions, testing$Volume)
```
Using only 1 variable and a linear regression the best results I obtained in forecasting the Volume was with the PositiveServiceReview feature. The RMSE is 984 and the R-squared is 0.33, which are two extremely high values revealing that this model is not effective and is not robust or accurate enough to be utilized with confidence.

Linear models, generalized linear models, and nonlinear models are examples of parametric regression models because we know the function that describes the relationship between the response and explanatory variables.
This dataset is small and the data does not seem to follow the distributional requirements of parametric methods as can be seen below, the data is highly skewed and many features are categorical, not numerical:

```{r - Histogram of all variables in the dataset}
hist(train_df$ProductTypeAccessories)
hist(train_df$ProductTypeDisplay)
hist(train_df$ProductTypeExtendedWarranty)
hist(train_df$ProductTypeGameConsole)
hist(train_df$ProductTypeLaptop)
hist(train_df$ProductTypeNetbook)
hist(train_df$ProductTypePC)
hist(train_df$ProductTypePrinter)
hist(train_df$ProductTypePrinterSupplies)
hist(train_df$ProductTypeSmartphone)
hist(train_df$ProductTypeSoftware)
hist(train_df$ProductTypeTablet)
hist(train_df$ProductNum)
hist(train_df$Price)
hist(train_df$x5StarReviews)
hist(train_df$x4StarReviews)
hist(train_df$x3StarReviews)
hist(train_df$x2StarReviews)
hist(train_df$x1StarReviews)
hist(train_df$PositiveServiceReview)
hist(train_df$NegativeServiceReview)
hist(train_df$Recommendproduct)
hist(train_df$ShippingWeight)
hist(train_df$ProductDepth)
hist(train_df$ProductWidth)
hist(train_df$ProductHeight)
hist(train_df$ProfitMargin)
hist(train_df$Volume)
```

Note: Below I created a multiple linear regression (which is not what the PoA asked for, therefore I will not go into much detail other than a brief comment.

```{r}
#Write trainSet in the data parameter instead of on each variable just to improve readability
MLRModel<- lm(Volume ~ ., training) #Creation of a Multiple Linear Regression taking into account all variables (except obviously for the dependent one) as features to forecast the Volume;
summary(MLRModel)

pred_MLR <- predict(MLRModel, newdata = testing)

postResample(pred_MLR, testing$Volume)
```

The multiple linear regression obtained incredible results which is doubtful. Something could be wrong above. If all is correct then the dependent variable is extremely predictable looking into the independent ones. I will confirm this in the following models.

*Let's dive into some non-parametric machine learning models now:*

```{r - Set seed & define training and test datasets}
set.seed(107)

#Train and test data sets have been previously created.

```


**Caret Model 2: Support Vector Machine (SVM)**
```{r - SVM 10 fold cross validation}
svmfitControl <- trainControl(method = "repeatedcv", 
                              number = 10, # number of folds
                              repeats = 1) #the number of complete sets of folds to compute
```
As asked, in this part we are supposed to train a model using Support Vector Machine, SVM, on the training set without overfitting. I decided to use 10-fold cross-validation and an Automatic Tuning Grid, which is what is coded below

In order to choose the optimization metric:
-Both RMSE and R2 quantify how well a regression model fits a dataset.
-The RMSE tells us how well a regression model can predict the value of the response variable in absolute terms while R2 tells us how well a model can predict the value of the response variable in percentage terms.

Therefore, I will use the RMSE metric as the point of this regression is to forecast the absolute Volume as accurately as possible (also the results were better utilizing this metric).

```{r - Train SVM Regression model}
svmFit1 <- train(Volume~., #y, target
                data = training,#X, features
                metric = 'RMSE', #Metric applied
                method = "svmLinear", #ML algorithm
                trControl=svmfitControl,  #Apply CV to the training
                tuneGrid = expand.grid(C=3**(-7:7)),
                verbose = FALSE)

svmFit1

```

Train function chooses the model with the largest performance value (or smallest, for mean squared error in regression models), therefore there is no need to select the best iteration of the models.

```{r -  Ascertain how the model prioritized each feature in the SVM training}
varImp(svmFit1)
```

```{r - Make predictions based on SVM trained model and calculate basic metrics (with PostResample)}
pred_SVM <- predict(svmFit1, newdata = testing)

Pred_Value_SVM <- predict(svmFit1, newdata = testing)

postResample(pred_SVM, testing$Volume)
```

For the testing portion of the train_df: 
The value of the Root Mean Squared Error is ~362, taking into account that the average Volume is 700, this error is not terrible, but it is not great as well. The same goes for a Mean Absolute Error of ~264.
The rsquared is ~0.77 which means that this models is an alright fit for this task.

**Caret Model 3: Random Forest (RF)**
```{r - RF 10 fold cross validation}
rffitControl <- trainControl(method = "repeatedcv", 
                              number = 10, # number of folds
                              repeats = 1) #the number of complete sets of folds to compute
```

As asked, in this part we are supposed to train a model using Random Forest, RF, on the training set without overfitting. I decided to use 10-fold cross-validation and an Automatic Tuning Grid, which is what is coded below.

I will use the RMSE metric for the same reasons as expressed above.

```{r - Create grid for manual training of mtry & train Random Forest Regression model}
rfFit1 <- train(Volume~., #y, target
                data = training,#X, features
                metric = 'RMSE', #Metric applied
                method = "rf", #ML algorithm
                trControl=rffitControl, #Apply CV to the training
                tuneLength=100, # Number of levels for each tuning parameters that should be generated
                verbose = FALSE)

rfFit1

```



```{r -  Ascertain how the model prioritized each feature in the RF training}
varImp(rfFit1)
```

Similarly to the previous model, the most relevant features are by far the ones associated with reviews (as predicted).

```{r - Make predictions based on RF trained model and calculate basic metrics (with PostResample)}
pred_RF <- predict(rfFit1, newdata = testing)

Pred_Value_RF <- predict(rfFit1, newdata = testing)

postResample(pred_RF, testing$Volume)
```

For the testing portion of the train_df: 
The value of the Root Mean Squared Error is ~528, taking into account that the average Volume is 700, this error is not terrible, but it is not great as well. The same goes for a Mean Absolute Error of ~165
The rsquared is ~0.79 which means that this models is a better fit for this task than the SVM.


**Caret Model 4: Gradient Boosting (GB)**

```{r - GBM 10 fold cross validation}
gbmfitControl <- trainControl(method = "repeatedcv", 
                              number = 10, # number of folds
                              repeats = 1) #the number of complete sets of folds to compute
```

As asked, in this part we are supposed to train a model using Gradient Boosting, GBM, on the training set without overfitting. I decided to use 10-fold cross-validation and an Automatic Tuning Grid, which is what is coded below.

I will use the RMSE metric for the same reasons as expressed above.

```{r - Train Gradient Boost Classification model}
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
gbmFit1 <- train(Volume~., #y, target
                 data = training, #X, features
                 metric = 'RMSE', #Metric applied
                 method = "gbm", #ML algorithm
                 trControl=gbmfitControl, #Apply CV to the training
                 tuneLength = 10, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

gbmFit1
```

As asked, in this part we are supposed to train a model using Stochastic Gradient Boosting, GBM, on the training set with 10-fold cross-validation and an Automatic Tuning Grid, which is what is coded above.

Train function chooses the model with the largest performance value (or smallest, for mean squared error in regression models), therefore there is no need to select the best iteration of the models.

```{r -  Ascertain how the model prioritized each feature in the GBM training}
varImp(gbmFit1)
```

As predicted the salary variable is the most relevant in predicting the target variable. Age comes in a close second place. The remaining variables have a really low relative impact when forecasting the target.

```{r - Make predictions based on GBM trained model and calculate basic metrics (with PostResample)}
pred_GBM <- predict(gbmFit1, newdata = testing)

Pred_Value_GBM <- predict(gbmFit1, newdata = testing)

postResample(pred_GBM, testing$Volume)
```

For the testing portion of the train_df: 
The value of the Root Mean Squared Error is ~672, taking into account that the average Volume is 700, this error is not terrible, but it is not great as well. The same goes for a Mean Absolute Error of ~551.
The rsquared is ~0.70.



*Compare the results from the 3 models:*

```{r - Compare results}
results <- resamples(list(SVM = svmFit1, GBM=gbmFit1, RF=rfFit1))

summary(results)

bwplot(results, metric = 'MAE')
bwplot(results, metric = 'RMSE')
bwplot(results, metric = 'Rsquared')
```
I have plotted the 3 metrics individually due an issue where the x window size is the same for all plots making the Rsquared plot useless.

Anyhow, looking firstly at the MAE plot, the RF is clearly the best performing model as it has the smallest mean average error.
The RMSE is not so clear. The GBM plot seems to be the worse. Between RF and SVM, RF has a much larger 3rd quartile value, but the median of the former is slightly smaller than SVM.
Finally, the Rsquared of the RF is clearly the best one.

In conclusion, I believe that the RF is the best performing model.


*Forecast testing dataset with the best performing model (GBM)*

The pre-process applied on train_df has been equally applied on test_df, therefore no further processing is needed to use this dataset.



```{r - Calculate the prediction with the best model trained above:}
prediction<-predict(rfFit1, test_df)
```


```{r - Post resampling for results of model in test dataset:}

postResample(prediction, test_df$Volume)

```

Not surprisingly the RMSE, Rsquared and MAE are absolutely awful. As mentioned before, the values for the target column of the test dataset seem to have been filled arbitrarily and should not be payed any attention. Therefore, we cannot measure the actual accuracy or any other metric in the test dataset, as there is no true column to compare the forecast against. We must believe that the population from the test dataset behaves similarly to the train dataset and that both datasets have been picked randomly (otherwise the population could have suffer sample selection bias)

```{r - Summary of prediction:}
summary(prediction)
```

```{r - Preferred computer brand comparison in the train, test and both datasets:}
hist(train_df$Volume)
#hist(test_df$Volume) #Useless
hist(prediction)
```


```{r}
output <- test_df

output$predictions <- prediction
write.csv(output, file="C2.T3output.csv", row.names = TRUE)
```




In conclusion, Blackwell's customers typically prefer Sony as their computer brand comparatively to Acer, and this can be forecast with a high accuracy.