---
title: "Getting started with R"
output: html_notebook
---

This notebook contains the answers to Task 2: Classification: Predict which Brand of Products Customers Prefer.


```{r - Initial imports}
library(readr)
library(caret)
library(ggplot2)
```

```{r - Read the dataset from memory}
train_df<- read.csv("CompleteResponses.csv")#Clients who included their favorite computer brand in the questionaire answers
test_df<- read.csv("SurveyIncomplete.csv") #Clients that did not favorite computer brand
train_df
test_df

```

These are the two datasets to be used for this task. 
It's worth noting that the column "brand" in the test_df contains either default or false information therefore it's useless and should not be considered.

*Understand the data:*

```{r - Getting to know the training data}
#print("List your attributes within your data set.")
#attributes(train_df)  # Too long
print("Prints the min, max, mean, median, and quartiles of each attribute.")
summary(train_df)
print("Displays the structure of your data set.")
str(train_df)
print("Names your attributes within your data set.")
names(train_df)
#print("Will print out the instances within that particular column in your data set.")
#train_df$salary # Too long
#train_df$age  # Too long
```

```{r - Compare if the testing data is similar to the training data (except for the brand column)}
print("Prints the min, max, mean, median, and quartiles of each attribute.")
summary(test_df)
print("Displays the structure of your data set.")
str(test_df)
print("Names your attributes within your data set.")
names(test_df)
```
The summary for each column is quite similar between these two dataframes.In fact, the minimum and maximum value are exactly the same for all columns except for the "brand" (which is comprehensible). The value for 1st and 3rd quartile as well as the mean and median is also pretty similar.


```{r - Do column types in train dataset coincide with their data? Or do I need to convert data types?}

#Convert categorical columns into factors to indicate that these columns contain a limited number of unique values.
train_df$elevel <- as.factor(train_df$elevel)
train_df$car <- as.factor(train_df$car)
train_df$zipcode <- as.factor(train_df$zipcode)
train_df$brand <- as.factor(train_df$brand)


sapply(train_df, typeof)
sapply(train_df, typeof) == sapply(test_df, typeof)

```
Yes, they have the same data type. All columns are of numerical type so no adjustment needed here. Convert only target column, brand, into factor, to make a binary classifier model, otherwise caret would assume variable is continuous.

```{r - Replicate same conversions to test_df:}
train_df$elevel <- as.factor(train_df$elevel)
train_df$car <- as.factor(train_df$car)
train_df$zipcode <- as.factor(train_df$zipcode)
test_df$brand <- as.factor(test_df$brand)

```

```{r - Are there missing values?}
any(is.na(train_df))
any(is.na(test_df))
```
There are no missing values in any of the dataframes, thus the chunk below is unnecessary.

```{r - Address missing values}
#na.omit(DatasetName$ColumnName#Drops any rows with missing values and omits them forever.
#na.exclude(DatasetName$ColumnName)#Drops any rows with missing values, but keeps track of where they were.
#DatasetName$ColumnName[is.na(DatasetName$ColumnName)]<-mean(DatasetName$ColumnName,na.rm = TRUE) #Replace the missing values with the mean
```



*Exploratory Data Analysis*

```{r - Histogram plots for train data}
#Salary
hist(train_df$salary,xlab="Salary",main="Distribution of salary amount")
#Age
hist(train_df$age,xlab="Age",main="Distribution of ages")
#Education
x<-barplot(table(train_df$elevel), xaxt="n")
text(cex=1, x=x-.25, y=-300, c('Less than HS', 'HS','Some College','4-y college','Mst/Doc/ProfD'), xpd=TRUE, srt=45)
#Car
x<-barplot(table(train_df$car), xaxt="n")
text(cex=1, x=x-.25, y=-100, c('BMW','Buick','Cadillac','Chevrolet','Chrysler','Dodge','Ford','Honda','Hyundai','Jeep','Kia','Lincoln','Mazda','Mercedes','Mitsubishi','Nissan','Ram','Subaru','Toyota','Other'), xpd=TRUE, srt=90)
#Zip Code
x<-barplot(table(train_df$zipcode), xaxt="n")
text(cex=1, x=x-.25, y=-200, c('New England','Mid-Atlantic','E-N-Central','W-N-Central','S-Atlantic','E-S-Central','W-S-Central','Mountain','Pacific'), xpd=TRUE, srt=60)
#Credit
hist(train_df$credit,xlab="Credit",main="Distribution of credit amount")
#Computer Brand
barplot(table(train_df$brand),names.arg = c('Acer','Sony'))
```

It might seem that there is a binning problem with the age variable, but after confirming the count of each value, these are the actual counts (with table(test_df$age)) in the dataframe. Age 20 and 80 have more clients than their neighbour ages.

```{r - Histogram plots for test data}
#Salary
hist(test_df$salary,xlab="Salary",main="Distribution of salary amount")
#Age
hist(test_df$age,xlab="Age",main="Distribution of ages")
#Education
x<-barplot(table(test_df$elevel), xaxt="n")
text(cex=1, x=x-.25, y=-150, c('Less than HS', 'HS','Some College','4-y college','Mst/Doc/ProfD'), xpd=TRUE, srt=45)
#Car
x<-barplot(table(test_df$car), xaxt="n")
text(cex=1, x=x-.25, y=-40, c('BMW','Buick','Cadillac','Chevrolet','Chrysler','Dodge','Ford','Honda','Hyundai','Jeep','Kia','Lincoln','Mazda','Mercedes','Mitsubishi','Nissan','Ram','Subaru','Toyota','Other'), xpd=TRUE, srt=90)
#Zip Code
x<-barplot(table(test_df$zipcode), xaxt="n")
text(cex=1, x=x-.25, y=-100, c('New England','Mid-Atlantic','E-N-Central','W-N-Central','S-Atlantic','E-S-Central','W-S-Central','Mountain','Pacific'), xpd=TRUE, srt=60)
#Credit
hist(test_df$credit,xlab="Credit",main="Distribution of credit amount")
#Computer Brand
barplot(table(test_df$brand),names.arg = c('Acer','Sony'))
```

Once again, it might seem that there is a binning problem with the age variable, but after confirming the count of each value (with table(test_df$age)), these are the actual counts in the dataframe. 


Apart from the column "brand", both train and test datasets have quite similar characteristics. This difference is alright because this column is faulty in the test dataset.

```{r - Scatter (Box) Plot}
plot(train_df$salary,train_df$age)
plot(train_df$salary,train_df$elevel)
plot(train_df$salary,train_df$car)
plot(train_df$salary,train_df$zipcode)
plot(train_df$salary,train_df$credit)
plot(train_df$salary,train_df$brand)

plot(train_df$age,train_df$elevel)
plot(train_df$age,train_df$car)
plot(train_df$age,train_df$zipcode)
plot(train_df$age,train_df$credit)
plot(train_df$age,train_df$brand)

plot(train_df$elevel,train_df$car)
plot(train_df$elevel,train_df$zipcode)
plot(train_df$elevel,train_df$credit)
plot(train_df$elevel,train_df$brand)

plot(train_df$car,train_df$zipcode)
plot(train_df$car,train_df$credit)
plot(train_df$car,train_df$brand)

plot(train_df$zipcode,train_df$credit)
plot(train_df$zipcode,train_df$brand)

plot(train_df$credit,train_df$brand)
```

None of the graphs above seem useful whatsoever..

```{r - Normal Quantile Plot for training dataset}
qqnorm(train_df$salary)
qqnorm(train_df$age)
qqnorm(as.integer(train_df$elevel))
qqnorm(as.integer(train_df$car))
qqnorm(as.integer(train_df$zipcode))
qqnorm(train_df$credit)
qqnorm(as.integer(train_df$brand))
```

```{r - Normal Quantile Plot for testing dataset}
qqnorm(test_df$salary)
qqnorm(test_df$age)
qqnorm(as.integer(test_df$elevel))
qqnorm(as.integer(test_df$car))
qqnorm(as.integer(test_df$zipcode))
qqnorm(test_df$credit)
qqnorm(as.integer(test_df$brand))
```


```{r - Basic boxplots to visualize relationship of training dataset}
ggplot(train_df, aes(x=as.factor(brand), y=salary)) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=age)) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(elevel))) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(car))) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(zipcode))) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=credit)) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
```

```{r - Basic boxplots to visualize relationship of testing dataset}
ggplot(test_df, aes(x=as.factor(brand), y=salary)) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=age)) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(elevel))) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(car))) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(zipcode))) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=credit)) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
```


*Creating testing and training sets with caret:*
```{r}
OneHot_train_df<- model.matrix(brand ~ ., data=train_df)
OneHot_train_df
```

```{r - Set seed & define training and test datasets}
set.seed(107)

OneHot_train_df<- head(model.matrix(brand ~ ., data=train_df))
OneHot_train_df

inTrain <- createDataPartition(y = train_df$brand,
            ## the outcome data are needed
            p = .75,# The percentage of data in the training set
            list = FALSE)
#The output is a set of integers for the rows of Sonar that belong in the training set.

training <- OneHot_train_df[ inTrain,] #TrainSet will contain the selected indices.
testing <- OneHot_train_df[-inTrain,] #TestSet will contain only the remaining rows that have not been selected.

nrow(training)
nrow(testing)

```


*Caret Model*

**Build a model with Stochastic Gradient Boosting GBM, with 10-fold-cross-validation and Automatic tuning Grid**
```{r - GBM 10 fold cross validation}
gbmfitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
```



```{r - Train Gradient Boost Classification model}
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
gbmFit1 <- train(brand~., data = training, method = "gbm", trControl=gbmfitControl, tuneLength = 10, verbose = FALSE)

gbmFit1
```


```{r -  Ascertain how the model prioritized each feature in the GBM training}
library(gbm)
varImp(gbmFit1)
```
```{r- Make predictions based on previously trained model}
predict()
```

```{r - Post resample for whatever}
postResample()?
```


**Use Random Forest with 10-fold cross-validation and manually tune 5 different mtry values**

mtry is the number of variables randomly sampled as candidates at each split. Note that the default values are different for classification (sqrt(p) where p is number of variables in x) and regression (p/3)

```{r - RF 10 fold cross validation}
rffitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
```


```{r - RF dataframe for manual tuning of mtry}
rfGrid <- expand.grid(mtry=c(1,2,3,4,5))
```


```{r - Train Random Forest Regression model}
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=rffitControl, verbose = FALSE, tuneGrid=rfGrid)

rfFit1
```


```{r -  Ascertain how the model prioritized each feature in the RF training}
varImp(rfFit1)
```