---
title: "Getting started with R"
output: html_notebook
---

This notebook contains the answers to Task 2: Classification: Predict which Brand of Products Customers Prefer.


```{r - Initial imports}
library(readr)
library(caret)
library(ggplot2)
library(dplyr)
```

```{r - Read the dataset from memory}
train_df<- read.csv("CompleteResponses.csv")#Clients who included their favorite computer brand in the questionaire answers
test_df<- read.csv("SurveyIncomplete.csv") #Clients that did not favorite computer brand
train_df
test_df
```

These are the two datasets to be used for this task. 
It's worth noting that the column "brand" in the test_df contains either default or false information therefore it's useless and should not be considered.

*Understand the data:*

```{r - Getting to know the training data}
#print("List your attributes within your data set.")
#attributes(train_df)  # Too long
print("Prints the min, max, mean, median, and quartiles of each attribute.")
summary(train_df)
print("Displays the structure of your data set.")
str(train_df)
print("Names your attributes within your data set.")
names(train_df)
#print("Will print out the instances within that particular column in your data set.")
#train_df$salary # Too long
#train_df$age  # Too long
```

```{r - Compare if the testing data is similar to the training data (except for the brand column)}
print("Prints the min, max, mean, median, and quartiles of each attribute.")
summary(test_df)
print("Displays the structure of your data set.")
str(test_df)
print("Names your attributes within your data set.")
names(test_df)
```
The summary for each column is quite similar between these two dataframes.In fact, the minimum and maximum value are exactly the same for all columns except for the "brand" (which is comprehensible). The value for 1st and 3rd quartile as well as the mean and median is also pretty similar.


```{r - Do column types in train dataset coincide with their data? Or do I need to convert data types?}

#Convert categorical columns into factors to indicate that these columns contain a limited number of unique values.
train_df$elevel <- as.factor(train_df$elevel)
train_df$car <- as.factor(train_df$car)
train_df$zipcode <- as.factor(train_df$zipcode)
train_df$brand <- as.factor(train_df$brand)

sapply(train_df, typeof)
sapply(train_df, typeof) == sapply(test_df, typeof)

```
Yes, they have the same data type. All columns are of numerical type so no adjustment needed here. 
In this project I will predict a nominal value, not a numerical one. Therefore I have to convert the target column (brand) into a factor. Otherwise caret would assume variable is continuous and would not make a binary classifier model.

I will do the same into the categorical features because as far as I investigated, categorical features should be factors, as these are not numerical and continuous, and consequently the final model will be better adjusted to reality and have a better performance.

```{r - Replicate same conversions to test_df:}

test_df$elevel <- as.factor(test_df$elevel)
test_df$car <- as.factor(test_df$car)
test_df$zipcode <- as.factor(test_df$zipcode)
test_df$brand <- as.factor(test_df$brand)

sapply(test_df, typeof)
sapply(test_df, typeof) == sapply(test_df, typeof)
```

```{r - Are there missing values?}
any(is.na(train_df))
any(is.na(test_df))
```
There are no missing values in any of the dataframes, thus the chunk below is unnecessary.

```{r - Address missing values}
#na.omit(DatasetName$ColumnName#Drops any rows with missing values and omits them forever.
#na.exclude(DatasetName$ColumnName)#Drops any rows with missing values, but keeps track of where they were.
#DatasetName$ColumnName[is.na(DatasetName$ColumnName)]<-mean(DatasetName$ColumnName,na.rm = TRUE) #Replace the missing values with the mean
```



*Exploratory Data Analysis*

```{r - Histogram plots for train data}
#Salary
hist(train_df$salary,xlab="Salary",main="Distribution of salary amount")
#Age
hist(train_df$age,xlab="Age",main="Distribution of ages")
#Education
x<-barplot(table(train_df$elevel), xaxt="n")
text(cex=1, x=x-.25, y=-300, c('Less than HS', 'HS','Some College','4-y college','Mst/Doc/ProfD'), xpd=TRUE, srt=45)
#Car
x<-barplot(table(train_df$car), xaxt="n")
text(cex=1, x=x-.25, y=-100, c('BMW','Buick','Cadillac','Chevrolet','Chrysler','Dodge','Ford','Honda','Hyundai','Jeep','Kia','Lincoln','Mazda','Mercedes','Mitsubishi','Nissan','Ram','Subaru','Toyota','Other'), xpd=TRUE, srt=90)
#Zip Code
x<-barplot(table(train_df$zipcode), xaxt="n")
text(cex=1, x=x-.25, y=-200, c('New England','Mid-Atlantic','E-N-Central','W-N-Central','S-Atlantic','E-S-Central','W-S-Central','Mountain','Pacific'), xpd=TRUE, srt=60)
#Credit
hist(train_df$credit,xlab="Credit",main="Distribution of credit amount")
#Computer Brand
barplot(table(train_df$brand),names.arg = c('Acer','Sony'))
```

Looking at the distributions above, all variables seem to be quite uniformly distributed. Nothing to point out in the independent variables.

The biggest exception is the target variable where there are almost twice as many clients with Sony relatively to Acer. The target variable is quite imbalanced.


```{r - Histogram plots for test data}
#Salary
hist(test_df$salary,xlab="Salary",main="Distribution of salary amount")
#Age
hist(test_df$age,xlab="Age",main="Distribution of ages")
#Education
x<-barplot(table(test_df$elevel), xaxt="n")
text(cex=1, x=x-.25, y=-150, c('Less than HS', 'HS','Some College','4-y college','Mst/Doc/ProfD'), xpd=TRUE, srt=45)
#Car
x<-barplot(table(test_df$car), xaxt="n")
text(cex=1, x=x-.25, y=-40, c('BMW','Buick','Cadillac','Chevrolet','Chrysler','Dodge','Ford','Honda','Hyundai','Jeep','Kia','Lincoln','Mazda','Mercedes','Mitsubishi','Nissan','Ram','Subaru','Toyota','Other'), xpd=TRUE, srt=90)
#Zip Code
x<-barplot(table(test_df$zipcode), xaxt="n")
text(cex=1, x=x-.25, y=-100, c('New England','Mid-Atlantic','E-N-Central','W-N-Central','S-Atlantic','E-S-Central','W-S-Central','Mountain','Pacific'), xpd=TRUE, srt=60)
#Credit
hist(test_df$credit,xlab="Credit",main="Distribution of credit amount")
#Computer Brand
barplot(table(test_df$brand),names.arg = c('Acer','Sony'))
```

The distribution of variables seem to be quite uniformly distributed. Once again, the target column stands out, because in the test dataset this column is faulty and is filled with 0 by default. Apart from the column "brand", both train and test datasets have quite similar characteristics.

```{r - Scatter (Box) Plot}
plot(train_df$salary,train_df$age)
plot(train_df$salary,train_df$elevel)
plot(train_df$salary,train_df$car)
plot(train_df$salary,train_df$zipcode)
plot(train_df$salary,train_df$credit)
plot(train_df$salary,train_df$brand)

plot(train_df$age,train_df$elevel)
plot(train_df$age,train_df$car)
plot(train_df$age,train_df$zipcode)
plot(train_df$age,train_df$credit)
plot(train_df$age,train_df$brand)

plot(train_df$elevel,train_df$car)
plot(train_df$elevel,train_df$zipcode)
plot(train_df$elevel,train_df$credit)
plot(train_df$elevel,train_df$brand)

plot(train_df$car,train_df$zipcode)
plot(train_df$car,train_df$credit)
plot(train_df$car,train_df$brand)

plot(train_df$zipcode,train_df$credit)
plot(train_df$zipcode,train_df$brand)

plot(train_df$credit,train_df$brand)
```

None of the graphs above seem useful whatsoever..

```{r - Normal Quantile Plot for training dataset}
qqnorm(train_df$salary)
qqnorm(train_df$age)
qqnorm(as.integer(train_df$elevel))
qqnorm(as.integer(train_df$car))
qqnorm(as.integer(train_df$zipcode))
qqnorm(train_df$credit)
qqnorm(as.integer(train_df$brand))
```

```{r - Normal Quantile Plot for testing dataset}
qqnorm(test_df$salary)
qqnorm(test_df$age)
qqnorm(as.integer(test_df$elevel))
qqnorm(as.integer(test_df$car))
qqnorm(as.integer(test_df$zipcode))
qqnorm(test_df$credit)
qqnorm(as.integer(test_df$brand))
```


```{r - Basic boxplots to visualize relationship of training dataset}
ggplot(train_df, aes(x=as.factor(brand), y=salary)) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=age)) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(elevel))) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(car))) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(zipcode))) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=credit)) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
```

The boxplots of each variable seem to be quite similar for any of the brands except in the salary's case, this might be the most revealing feature.

```{r - Basic boxplots to visualize relationship of testing dataset}
ggplot(test_df, aes(x=as.factor(brand), y=salary)) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=age)) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(elevel))) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(car))) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(zipcode))) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=credit)) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("brand")
```
Both salary and age features have quite different boxplots for each brand, these two features seem to be the most relevant to forecast the brand in the test dataset.


In conclusion, it seems that both datasets have quite similar characteristics and the model trained in the train dataset will be useful in forecasting the same feature of the test dataset.

*Creating testing and training sets with caret:*
```{r - Make dummy variable for caret}
OneHot_train_df <- data.frame(model.matrix( ~ ., data=train_df))
colnames(OneHot_train_df)[colnames(OneHot_train_df) == 'brand1'] <- 'brand'

OneHot_train_df$brand <- as.factor(OneHot_train_df$brand) #Reconvert brand into binary column not integer.

OneHot_train_df 
```
From what I read caret requires (or at least performs better) with dummy features. Therefore I converted the categorical variables into dummy features but kept the target variable (brand) as it was.

```{r - Set seed & define training and test datasets}
set.seed(107)

inTrain <- createDataPartition(y = OneHot_train_df$brand,
            ## the outcome data are needed
            p = .75,# The percentage of data in the training set should be 75%
            list = FALSE)
#The output is a set of integers for the rows of Sonar that belong in the training set.

training <- OneHot_train_df[ inTrain,] #TrainSet will contain the selected indices.
testing <- OneHot_train_df[-inTrain,] #TestSet will contain only the remaining rows that have not been selected.

nrow(training)
nrow(testing)

```

```{r - Load missing libraries for the modeling part of this notebook}
library(gbm)
library(Rcpp)
```


**Caret Model 1: Build a model with Stochastic Gradient Boosting GBM to predict nominal data, with 10-fold-cross-validation and Automatic tuning Grid**
```{r - GBM 10 fold cross validation}
gbmfitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
```

Because this dataset is slightly unbalanced, I will attempt to train according to the kappa metric instead of accuracy.
(Practically, Cohen’s kappa removes the possibility of the classifier and a random guess agreeing and measures the number of predictions it makes that cannot be explained by a random guess. Furthermore, Cohen’s kappa tries to correct the evaluation bias by taking into account the correct classification by a random guess.)

```{r - Train Gradient Boost Classification model}
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
gbmFit1 <- train(brand~., #y, target
                 data = training, #X, features
                 metric = 'Kappa', #Metric applied
                 method = "gbm", #ML algorithm
                 trControl=gbmfitControl, #Apply CV to the training
                 tuneLength = 10, # Number of levels for each tuning parameters that should be generated
                 verbose = FALSE)

gbmFit1
```

As asked, in this part we are supposed to train a model using Stochastic Gradient Boosting, GBM, on the training set with 10-fold cross-validation and an Automatic Tuning Grid, which is what is coded above.

Train function chooses the model with the largest performance value (or smallest, for mean squared error in regression models), therefore there is no need to select the best iteration of the models.

```{r -  Ascertain how the model prioritized each feature in the GBM training}
varImp(gbmFit1)
```

As predicted the salary variable is the most relevant in predicting the target variable. Age comes in a close second place. The remaining variables have a really low relative impact when forecasting the target.

```{r - Make predictions based on GBM trained model and calculate basic metrics (with PostResample)}
pred_GBM <- predict(gbmFit1, newdata = testing)

Prob_pred_GBM <- predict(gbmFit1, newdata = testing, type = "prob")

postResample(pred_GBM, testing$brand)
```

This model provides an accuracy of ~94% which is pretty high.
The Kappa or Cohen's Kappa is at ~86% which is quite impressive since this metric is normalized at the baseline of random chance on our dataset.
This dataset has a slightly unbalanced target, therefore the Kappa value is quite different from the accuracy value.
It's worth adding that if the model was trained to optimise the accuracy, the metrics accuracy and Kappa obtained would be respectively: 0.932498 0.856289. Therefore optimising this model to the metric "Kappa" is a much better alternative for this dataset as it provided better results in both metrics!


```{r - Further metrics with confusion matrix}
confusionMatrix(data = pred_GBM, 
                reference = testing$brand, 
                positive = "1")
```
The confusion matrix is quite solid, the auxiliary statistical metrics derived, seem to confirm so. Below, I commented how each of the metrics calculated from the confusion matrix support that the model has a good performance overall:
- _Accuracy_: This is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations ((TN+TP)/Total). Forecasting correctly ~94% of the test observations is pretty good;
- _95% CI_: The 95% confidence interval estimates that the accuracy falls within an interval with low variation within a high performance.
- _No information rate_: The "no-information rate" is the largest proportion of the observed classes. The test set contains 62% class 1 (Sony) and only (100-62)=38% class 0 (Acer);
- _P-Value_: A p-value, or probability value, is a number describing how likely it is that your data would have occurred by random chance (i.e. that the null hypothesis is true). In this case the p-value is clearly smaller than the the reference of 0.05, meaning that the p-value is statistically significant and the null hypothesis may be rejected;

- _Kappa_: The Kappa has been explained and commented above;

- _Mcnemar's Test P-Value_: In terms of comparing two binary classification algorithms, the test is commenting on whether the two models disagree in the same way (or not). It is not commenting on whether one model is more or less accurate or error prone than another. This is clear when we look at how the statistic is calculated. In this case we have fed a confusion matrix to the function and not a contingency table (comparing simply the number of correct vs. incorrect observations while discounting the classes);

- _Sensitivity_: Proportion of those who correctly received a positive result (class 1 - Sony) on this test out of those who actually have a positive result (TP/(TP+FN)). The value obtained for this metric is ~95% which is alright;
- _Specificity_: Proportion of those who correctly received a negative result (class 0 - Acer) on this test out of those who actually have a negative result (TN/(FP+TN)). The percentage of correctly predicted Sony is higher than Acer;
- _Pos Pred Value_: Proportion of positive results that are true positive (TP/(TP+FP));
- _Neg Pred Value_: Proportion of negative results that are true negative (TN/(TN+FN)). This model has a higher accuracy in forecasting class 1 (Sony) relatively to class 0;
- _Prevalence_:  Measure of the frequency of a condition in a population at a particular point in time ((TP+FN)/Total). In this case the condition measured is the positives. As can be seen this dataset is slightly unbalanced;
- _Detection Rate_: Here the detection rate is defined as the fraction of observations who are in fact positive and are called positive by the model (TP/Total). Fortunately, this percentage is similar to the prevalence meaning that the model captured the class skewness of this dataset. If the value was too different the False Negatives would be hindering the model's performance;
- _Detection Prevalence_: This is defined as the number of predicted positive events (both true positive and false positive) divided by the total number of predictions ((TP+FP)/Total). This value is similar to what was obtained in the prevalence metric, this indicates that the amount of false negatives is simillar to the false positives when considering the whole test dataset;
- _Balanced Accuracy_: Balanced accuracy attempts to account for the imbalance in classes ((TPR+TNR)/2 = (Sensitivity+Specificity)/2). The balanced accuracy is slightly lower than the accuracy. Nonetheless, this metric is at 92%, thus this model clearly captured the data unbalance and is not just supposing that the most frequent class is the correct one (in this case the balanced accuracy would be 50% and the accuracy would coincide with the proportion of the most frequent class relatively to the less occuring one).

```{r - Calculate two class summary for binary GBM model}
temp_pred_GB = factor(ifelse(pred_GBM == 1, "Y", "N"))
temp_obs_GB = factor(ifelse(testing$brand == 1, "Y", "N"))

twoClassSummary(data = data.frame(obs = temp_obs_GB, pred = temp_pred_GB, Y = Prob_pred_GBM$'1', N = Prob_pred_GBM$'0'), lev = levels(temp_pred_GB))
#For some reason, this function does not seem to provide ROC results if a binary numerical labeling is used, I had to convert '1' into 'Y' and '0' into 'N' to obtain valid results

mnLogLoss(data = data.frame(obs = temp_obs_GB, pred = temp_pred_GB, Y = Prob_pred_GBM$'1', N = Prob_pred_GBM$'0'), lev = levels(temp_pred_GB))
```
The area under the ROC curve is close to 1 meaning that the classifier is far better than a random classifier (AUC of 0.5). In fact, the AUC is close to 1, a perfect classifier. The value of the specificity and specificity seem to have swapped among themselves (the values calculated in the function above are the correct ones), thus, I will not comment these values.
- _Log Loss_: Log-loss is indicative of how close the prediction probability is to the corresponding actual/true value (0 or 1 in case of binary classification). The more the predicted probability diverges from the actual value, the higher is the log-loss value. In the case of the LogLoss metric, one usual "well-known" metric is to say that 0.693 is the non-informative value. This figure is obtained by predicting p = 0.5 for any class of a binary problem. This is valid only for balanced binary problems. 
Because we have obtained a Log Loss of 0.218, much lower than 0.693, we can afirm that this model is clearly better than random picking and has captured the problem.

**Caret Model 2: Use Random Forest with 10-fold cross-validation to predict nominal data and manually tune 5 different mtry values**

```{r - RF 10 fold cross validation}
rffitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
```

In this task we have to manually tune 5 different values for the "mtry" parameter.The "mtry" is the number of variables randomly sampled as candidates at each split. Note that the default values are different for classification (sqrt(p) where p is number of variables in x) and regression (p/3). In this case we are foing a classification thus the default value is sqrt(36) which is 6. I started by centering center the grid search in the default value and search equidistantly on both sides (upper and lower) of this value. However, I verified that the optimal performance falls in a higher value of "mtry".

Similarly to the model trained above, I will optimise towards the Kappa metric.

```{r - Create grid for manual training of mtry & train Random Forest Regression model}
rfGrid <- expand.grid(mtry=c(6,9,10,11,12))

# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., #y, target
                data = training,#X, features
                metric = 'Kappa', #Metric applied
                method = "rf", #ML algorithm
                trControl=rffitControl, #Apply manual grid search to the training
                tuneGrid=rfGrid, #Grid parameters
                verbose = FALSE)

rfFit1
```

As asked, in this part we are supposed to train a model using Random Forest, RF, on the training set with 10-fold cross-validation and an manual Tuning Grid, which is what is coded above.

Train function chooses the model with the largest performance value (or smallest, for mean squared error in regression models), therefore there is no need to select the best iteration of the models.


```{r -  Ascertain how the model prioritized each feature in the RF training}
varImp(rfFit1)
```

Similarly to the previous model, salary once again is the most relevant feature in predicting the target variable. Again, age comes in a second place. Credit has a much higher relative importance, however, it is clearly less important than the two top features. The remaining variables have a really low relative impact when forecasting the target.

```{r - Make predictions based on RF trained model and calculate basic metrics (wuth PostResample)}
pred_RF <- predict(rfFit1, newdata = testing)

Prob_pred_RF <- predict(rfFit1, newdata = testing, type = "prob")

postResample(pred_RF, testing$brand)

```

This model provides an accuracy of ~94% which is pretty high.
This dataset has a slightly imbalanced target, therefore the Kappa value is quite different from the accuracy value.
The Kappa or Cohen's Kappa is at ~86% which is quite impressive since this metric is normalized at the baseline of random chance on our dataset.


```{r - Further metrics of RF with confusion matrix}
confusionMatrix(data = pred_RF, 
                reference = testing$brand, 
                positive = "1")
```
Similarly to the previous model, the confusion matrix is quite solid, the auxiliary statistical metrics derived, seem to confirm so. The two models created in this exercise are quite similar in theory and in the provided results. Therefore, instead of repeating the comment section I have written for the previous model, I will just say that those comments can be applied to this model and move on to the next part (Additionaly, this model is slightly inferior relatively to the GBM thus I chose to focus in the other).


```{r - Calculate two class summary for binary RF model}
temp_pred_RF = factor(ifelse(pred_RF == 1, "Y", "N"))
temp_obs_RF = factor(ifelse(testing$brand == 1, "Y", "N"))

twoClassSummary(data = data.frame(obs = temp_obs_RF, pred = temp_pred_RF, Y = Prob_pred_RF$'1', N = Prob_pred_RF$'0'), lev = levels(temp_pred_RF))
#For some reason, this function does not seem to provide ROC results if a binary numerical labeling is used, I had to convert '1' into 'Y' and '0' into 'N' to obtain valid results


mnLogLoss(data = data.frame(obs = temp_obs_RF, pred = temp_pred_RF, Y = Prob_pred_RF$'1', N = Prob_pred_RF$'0'), lev = levels(temp_pred_RF))
```

The area under the ROC curve is close to 1 meaning that the classifier is far better than a random classifier (AUC of 0.5). In fact, the AUC is close to 1, a perfect classifier. The value of the specificity and specificity seem to have swapped among themselves (the values calculated in the function above are the correct ones), thus, I will not comment these values.
- _Log Loss_: Log-loss is indicative of how close the prediction probability is to the corresponding actual/true value (0 or 1 in case of binary classification). The more the predicted probability diverges from the actual value, the higher is the log-loss value. In the case of the LogLoss metric, one usual "well-known" metric is to say that 0.693 is the non-informative value. This figure is obtained by predicting p = 0.5 for any class of a binary problem. This is valid only for balanced binary problems. 
Because we have obtained a Log Loss of 0.218, much lower than 0.693, we can afirm that this model is clearly better than random picking and has captured the problem.

*Compare the results from two models:*
```{r - Compare results}
results <- resamples(list(GBMAut=gbmFit1, RFMan=rfFit1))

summary(results)

bwplot(results)
```
Looking at the table and boxplot of the chunk above it seems that both algorithms have a good performance in forecasting the target of this problem. However, the Stochastic Gradient Boost seems to provide a slightly better performing model as the interquartile range for both the accuracy and kappa falls is located at a higher value.


*Forecast testing dataset with the best performing model (GBM)*


```{r - Make dummy variable for caret}
OneHot_test_df <- data.frame(model.matrix( ~ ., data=test_df))
OneHot_test_df <- subset(OneHot_test_df, select=-c(brand1)) # Remove target column
```


```{r - Calculate the prediction with the best model trained above:}
prediction<-predict(gbmFit1,OneHot_test_df)
```


```{r - Post resampling for accuracy and Kappa of model in test dataset:}

postResample(prediction, test_df$brand)

```

Not surprisingly the Accuracy and Kappa are absolutely awful. As mentioned before, the values for the target column of the test dataset seem to have been filled arbitrarily and should not be payed any attention. Therefore, we cannot measure the actual accuracy or any other metric in the test dataset, as there is no true column to compare the forecast against. We must believe that the population from the test dataset behaves similarly to the train dataset and that both datasets have been picked randomly (otherwise the population could have suffer sample selection bias)

```{r - Summary of prediction:}
summary(prediction)
```

```{r - Preferred computer brand comparison in the train, test and both datasets:}
#Computer Brand
barplot(table(train_df$brand),names.arg = c('Acer','Sony'))

barplot(table(prediction),names.arg = c('Acer','Sony'))

TotalTarget <- data.frame(TestTrain = c(train_df$brand, prediction))
barplot(table(TotalTarget),names.arg = c('Acer','Sony'))

```


```{r - Plot chart to compare proportion of computer brands between Train and predictions}
# create a dataset
Dataset <- c("Train Dataset", "Predictions","Train Dataset", "Predictions")
Brand <- c("Sony","Sony","Acer","Acer")
PercentageCount <- c(table(train_df$brand)["0"],
           table(train_df$brand)["1"],
           table(prediction)["0"],
           table(prediction)["1"])
data <- data.frame(Dataset,Brand,Count)

# Stacked + percent
ggplot(data, aes(fill=Brand, y=PercentageCount, x=Dataset)) + 
    geom_bar(position="fill", stat="identity") +
    ylab("Percentage Count")
```
As can be seen above, the proportion of preferred computer brand is the same for both datasets, this supports the thesis that both datasets refer to a similarlly behaving population (if no the same) and that the model trained in the first dataset can be applied on the second.

```{r - Add column with predictions and save into .tab file}

test_df$prediction <- prediction
test_df
write_tsv(test_df,"SurveyIncomplete_WithPreds.tab")
```
