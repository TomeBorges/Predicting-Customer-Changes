qqnorm(as.integer(train_df$car))
qqnorm(as.integer(train_df$zipcode))
qqnorm(train_df$credit)
qqnorm(as.integer(train_df$brand))
qqnorm(test_df$salary)
qqnorm(test_df$age)
qqnorm(as.integer(test_df$elevel))
qqnorm(as.integer(test_df$car))
qqnorm(as.integer(test_df$zipcode))
qqnorm(test_df$credit)
qqnorm(as.integer(test_df$brand))
ggplot(train_df, aes(x=as.factor(brand), y=salary)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=age)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(elevel))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(car))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(zipcode))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=credit)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=salary)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=age)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(elevel))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(car))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(zipcode))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=credit)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
OneHot_train_df <- data.frame(model.matrix( ~ ., data=train_df))
colnames(OneHot_train_df)[colnames(OneHot_train_df) == 'brand1'] <- 'brand'
OneHot_train_df$brand <- as.factor(OneHot_train_df$brand) #Reconvert brand into binary column not integer.
OneHot_train_df
set.seed(107)
inTrain <- createDataPartition(y = OneHot_train_df$brand,
## the outcome data are needed
p = .75,# The percentage of data in the training set should be 75%
list = FALSE)
#The output is a set of integers for the rows of Sonar that belong in the training set.
training <- OneHot_train_df[ inTrain,] #TrainSet will contain the selected indices.
testing <- OneHot_train_df[-inTrain,] #TestSet will contain only the remaining rows that have not been selected.
nrow(training)
nrow(testing)
library(gbm)
library(Rcpp)
gbmfitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
gbmFit1 <- train(brand~., data = training, method = "gbm", trControl=gbmfitControl, tuneLength = 10, verbose = FALSE)
gbmFit1
library(gbm)
library(Rcpp)
library(readr)
library(caret)
library(ggplot2)
library(dplyr)
train_df<- read.csv("CompleteResponses.csv")#Clients who included their favorite computer brand in the questionaire answers
test_df<- read.csv("SurveyIncomplete.csv") #Clients that did not favorite computer brand
train_df
test_df
#print("List your attributes within your data set.")
#attributes(train_df)  # Too long
print("Prints the min, max, mean, median, and quartiles of each attribute.")
summary(train_df)
print("Displays the structure of your data set.")
str(train_df)
print("Names your attributes within your data set.")
names(train_df)
#print("Will print out the instances within that particular column in your data set.")
#train_df$salary # Too long
#train_df$age  # Too long
print("Prints the min, max, mean, median, and quartiles of each attribute.")
summary(test_df)
print("Displays the structure of your data set.")
str(test_df)
print("Names your attributes within your data set.")
names(test_df)
#Convert categorical columns into factors to indicate that these columns contain a limited number of unique values.
train_df$elevel <- as.factor(train_df$elevel)
train_df$car <- as.factor(train_df$car)
train_df$zipcode <- as.factor(train_df$zipcode)
train_df$brand <- as.factor(train_df$brand)
sapply(train_df, typeof)
sapply(train_df, typeof) == sapply(test_df, typeof)
train_df$elevel <- as.factor(train_df$elevel)
train_df$car <- as.factor(train_df$car)
train_df$zipcode <- as.factor(train_df$zipcode)
test_df$brand <- as.factor(test_df$brand)
any(is.na(train_df))
any(is.na(test_df))
#na.omit(DatasetName$ColumnName#Drops any rows with missing values and omits them forever.
#na.exclude(DatasetName$ColumnName)#Drops any rows with missing values, but keeps track of where they were.
#DatasetName$ColumnName[is.na(DatasetName$ColumnName)]<-mean(DatasetName$ColumnName,na.rm = TRUE) #Replace the missing values with the mean
#Salary
hist(train_df$salary,xlab="Salary",main="Distribution of salary amount")
#Age
hist(train_df$age,xlab="Age",main="Distribution of ages")
#Education
x<-barplot(table(train_df$elevel), xaxt="n")
text(cex=1, x=x-.25, y=-300, c('Less than HS', 'HS','Some College','4-y college','Mst/Doc/ProfD'), xpd=TRUE, srt=45)
#Car
x<-barplot(table(train_df$car), xaxt="n")
text(cex=1, x=x-.25, y=-100, c('BMW','Buick','Cadillac','Chevrolet','Chrysler','Dodge','Ford','Honda','Hyundai','Jeep','Kia','Lincoln','Mazda','Mercedes','Mitsubishi','Nissan','Ram','Subaru','Toyota','Other'), xpd=TRUE, srt=90)
#Zip Code
x<-barplot(table(train_df$zipcode), xaxt="n")
text(cex=1, x=x-.25, y=-200, c('New England','Mid-Atlantic','E-N-Central','W-N-Central','S-Atlantic','E-S-Central','W-S-Central','Mountain','Pacific'), xpd=TRUE, srt=60)
#Credit
hist(train_df$credit,xlab="Credit",main="Distribution of credit amount")
#Computer Brand
barplot(table(train_df$brand),names.arg = c('Acer','Sony'))
#Salary
hist(test_df$salary,xlab="Salary",main="Distribution of salary amount")
#Age
hist(test_df$age,xlab="Age",main="Distribution of ages")
#Education
x<-barplot(table(test_df$elevel), xaxt="n")
text(cex=1, x=x-.25, y=-150, c('Less than HS', 'HS','Some College','4-y college','Mst/Doc/ProfD'), xpd=TRUE, srt=45)
#Car
x<-barplot(table(test_df$car), xaxt="n")
text(cex=1, x=x-.25, y=-40, c('BMW','Buick','Cadillac','Chevrolet','Chrysler','Dodge','Ford','Honda','Hyundai','Jeep','Kia','Lincoln','Mazda','Mercedes','Mitsubishi','Nissan','Ram','Subaru','Toyota','Other'), xpd=TRUE, srt=90)
#Zip Code
x<-barplot(table(test_df$zipcode), xaxt="n")
text(cex=1, x=x-.25, y=-100, c('New England','Mid-Atlantic','E-N-Central','W-N-Central','S-Atlantic','E-S-Central','W-S-Central','Mountain','Pacific'), xpd=TRUE, srt=60)
#Credit
hist(test_df$credit,xlab="Credit",main="Distribution of credit amount")
#Computer Brand
barplot(table(test_df$brand),names.arg = c('Acer','Sony'))
plot(train_df$salary,train_df$age)
plot(train_df$salary,train_df$elevel)
plot(train_df$salary,train_df$car)
plot(train_df$salary,train_df$zipcode)
plot(train_df$salary,train_df$credit)
plot(train_df$salary,train_df$brand)
plot(train_df$age,train_df$elevel)
plot(train_df$age,train_df$car)
plot(train_df$age,train_df$zipcode)
plot(train_df$age,train_df$credit)
plot(train_df$age,train_df$brand)
plot(train_df$elevel,train_df$car)
plot(train_df$elevel,train_df$zipcode)
plot(train_df$elevel,train_df$credit)
plot(train_df$elevel,train_df$brand)
plot(train_df$car,train_df$zipcode)
plot(train_df$car,train_df$credit)
plot(train_df$car,train_df$brand)
plot(train_df$zipcode,train_df$credit)
plot(train_df$zipcode,train_df$brand)
plot(train_df$credit,train_df$brand)
qqnorm(train_df$salary)
qqnorm(train_df$age)
qqnorm(as.integer(train_df$elevel))
qqnorm(as.integer(train_df$car))
qqnorm(as.integer(train_df$zipcode))
qqnorm(train_df$credit)
qqnorm(as.integer(train_df$brand))
qqnorm(test_df$salary)
qqnorm(test_df$age)
qqnorm(as.integer(test_df$elevel))
qqnorm(as.integer(test_df$car))
qqnorm(as.integer(test_df$zipcode))
qqnorm(test_df$credit)
qqnorm(as.integer(test_df$brand))
ggplot(train_df, aes(x=as.factor(brand), y=salary)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=age)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(elevel))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(car))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(zipcode))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=credit)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=salary)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=age)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(elevel))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(car))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(zipcode))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=credit)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
OneHot_train_df <- data.frame(model.matrix( ~ ., data=train_df))
colnames(OneHot_train_df)[colnames(OneHot_train_df) == 'brand1'] <- 'brand'
OneHot_train_df$brand <- as.factor(OneHot_train_df$brand) #Reconvert brand into binary column not integer.
OneHot_train_df
set.seed(107)
inTrain <- createDataPartition(y = OneHot_train_df$brand,
## the outcome data are needed
p = .75,# The percentage of data in the training set should be 75%
list = FALSE)
#The output is a set of integers for the rows of Sonar that belong in the training set.
training <- OneHot_train_df[ inTrain,] #TrainSet will contain the selected indices.
testing <- OneHot_train_df[-inTrain,] #TestSet will contain only the remaining rows that have not been selected.
nrow(training)
nrow(testing)
gbmfitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
gbmFit1 <- train(brand~., data = training, method = "gbm", trControl=gbmfitControl, tuneLength = 10, verbose = FALSE)
gbmFit1
varImp(gbmFit1)
temp_pred_GB = factor(ifelse(pred_GBM == 1, "Y", "N"))
pred_GBM <- predict(gbmFit1, newdata = testing)
Prob_pred_GBM <- predict(gbmFit1, newdata = testing, type = "prob")
postResample(pred_GBM, testing$brand)
confusionMatrix(data = pred_GBM,
reference = testing$brand,
positive = "1")
temp_pred_GB = factor(ifelse(pred_GBM == 1, "Y", "N"))
temp_obs_GB = factor(ifelse(testing$brand == 1, "Y", "N"))
twoClassSummary(data = data.frame(obs = temp_obs_GB, pred = temp_pred_GB, Y = Prob_pred_GBM$'1', N = Prob_pred_GBM$'0'), lev = levels(temp_pred_GB))
#For some reason, this function does not seem to provide ROC results if a binary numerical labeling is used, I had to convert '1' into 'Y' and '0' into 'N' to obtain valid results
mnLogLoss(data = data.frame(obs = temp_obs_GB, pred = temp_pred_GB, Y = Prob_pred_GBM$'1', N = Prob_pred_GBM$'0'), lev = levels(temp_pred_GB))
rffitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
rffitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
rfGrid <- expand.grid(mtry=c(1,2,3,4,5))
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=rffitControl, verbose = FALSE, tuneGrid=rfGrid)
rfFit1
rfGrid <- expand.grid(mtry=c(3,4,5,6,7))
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=rffitControl, verbose = FALSE, tuneGrid=rfGrid)
rfFit1
training
rfGrid <- expand.grid(mtry=c(4,5,6,7,8))
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=rffitControl, verbose = FALSE, tuneGrid=rfGrid)
rfFit1
rfGrid <- expand.grid(mtry=c(5,6,7,8,9))
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=rffitControl, verbose = FALSE, tuneGrid=rfGrid)
rfFit1
rfGrid <- expand.grid(mtry=c(8,9,10,11,12))
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=rffitControl, verbose = FALSE, tuneGrid=rfGrid)
rfFit1
rfGrid <- expand.grid(mtry=c(6,9,10,11,12))
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., data = training, method = "rf", trControl=rffitControl, verbose = FALSE, tuneGrid=rfGrid)
rfFit1
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
gbmFit1 <- train(brand~., #y, target
data = training, #X, features
metric = 'kappa', #Metric applied
method = "gbm", #ML algorithm
trControl=gbmfitControl, #Apply CV to the training
tuneLength = 10, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
gbmFit1 <- train(brand~., #y, target
data = training, #X, features
metric = 'Kappa', #Metric applied
method = "gbm", #ML algorithm
trControl=gbmfitControl, #Apply CV to the training
tuneLength = 10, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
gbmFit1
varImp(gbmFit1)
pred_GBM <- predict(gbmFit1, newdata = testing)
Prob_pred_GBM <- predict(gbmFit1, newdata = testing, type = "prob")
postResample(pred_GBM, testing$brand)
confusionMatrix(data = pred_GBM,
reference = testing$brand,
positive = "1")
temp_pred_GB = factor(ifelse(pred_GBM == 1, "Y", "N"))
temp_obs_GB = factor(ifelse(testing$brand == 1, "Y", "N"))
twoClassSummary(data = data.frame(obs = temp_obs_GB, pred = temp_pred_GB, Y = Prob_pred_GBM$'1', N = Prob_pred_GBM$'0'), lev = levels(temp_pred_GB))
#For some reason, this function does not seem to provide ROC results if a binary numerical labeling is used, I had to convert '1' into 'Y' and '0' into 'N' to obtain valid results
mnLogLoss(data = data.frame(obs = temp_obs_GB, pred = temp_pred_GB, Y = Prob_pred_GBM$'1', N = Prob_pred_GBM$'0'), lev = levels(temp_pred_GB))
rffitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
rfGrid <- expand.grid(mtry=c(6,9,10,11,12))
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
rfFit1 <- train(brand~., #y, target
data = training,#X, features
metric = 'Kappa', #Metric applied
method = "rf", #ML algorithm
trControl=rffitControl, #Apply manual grid search to the training
tuneGrid=rfGrid, #Grid parameters
verbose = FALSE)
rfFit1
varImp(rfFit1)
pred_RF <- predict(rfFit1, newdata = testing)
Prob_pred_RF <- predict(rfFit1, newdata = testing, type = "prob")
postResample(pred_RF, testing$brand)
confusionMatrix(data = pred_RF,
reference = testing$brand,
positive = "1")
temp_pred_RF = factor(ifelse(pred_RF == 1, "Y", "N"))
temp_obs_RF = factor(ifelse(testing$brand == 1, "Y", "N"))
twoClassSummary(data = data.frame(obs = temp_obs_RF, pred = temp_pred_RF, Y = Prob_pred_RF$'1', N = Prob_pred_RF$'0'), lev = levels(temp_pred_RF))
#For some reason, this function does not seem to provide ROC results if a binary numerical labeling is used, I had to convert '1' into 'Y' and '0' into 'N' to obtain valid results
mnLogLoss(data = data.frame(obs = temp_obs_RF, pred = temp_pred_RF, Y = Prob_pred_RF$'1', N = Prob_pred_RF$'0'), lev = levels(temp_pred_RF))
results <- resamples(list(GBMAut=gbmFit1, RFMan=rfFit1))
summary(results)
bwplot(results)
temp_pred_GB = factor(ifelse(pred_GBM == 1, "Y", "N"))
temp_obs_GB = factor(ifelse(testing$brand == 1, "Y", "N"))
twoClassSummary(data = data.frame(obs = temp_obs_GB, pred = temp_pred_GB, Y = Prob_pred_GBM$'0', N = Prob_pred_GBM$'1'), lev = levels(temp_pred_GB))
library(gbm)
library(Rcpp)
library(readr)
library(caret)
library(ggplot2)
library(dplyr)
train_df<- read.csv("CompleteResponses.csv")#Clients who included their favorite computer brand in the questionaire answers
test_df<- read.csv("SurveyIncomplete.csv") #Clients that did not favorite computer brand
train_df
test_df
#print("List your attributes within your data set.")
#attributes(train_df)  # Too long
print("Prints the min, max, mean, median, and quartiles of each attribute.")
summary(train_df)
print("Displays the structure of your data set.")
str(train_df)
print("Names your attributes within your data set.")
names(train_df)
#print("Will print out the instances within that particular column in your data set.")
#train_df$salary # Too long
#train_df$age  # Too long
print("Prints the min, max, mean, median, and quartiles of each attribute.")
summary(test_df)
print("Displays the structure of your data set.")
str(test_df)
print("Names your attributes within your data set.")
names(test_df)
#Convert categorical columns into factors to indicate that these columns contain a limited number of unique values.
train_df$elevel <- as.factor(train_df$elevel)
train_df$car <- as.factor(train_df$car)
train_df$zipcode <- as.factor(train_df$zipcode)
train_df$brand <- as.factor(train_df$brand)
sapply(train_df, typeof)
sapply(train_df, typeof) == sapply(test_df, typeof)
train_df$elevel <- as.factor(train_df$elevel)
train_df$car <- as.factor(train_df$car)
train_df$zipcode <- as.factor(train_df$zipcode)
test_df$brand <- as.factor(test_df$brand)
any(is.na(train_df))
any(is.na(test_df))
#na.omit(DatasetName$ColumnName#Drops any rows with missing values and omits them forever.
#na.exclude(DatasetName$ColumnName)#Drops any rows with missing values, but keeps track of where they were.
#DatasetName$ColumnName[is.na(DatasetName$ColumnName)]<-mean(DatasetName$ColumnName,na.rm = TRUE) #Replace the missing values with the mean
#Salary
hist(train_df$salary,xlab="Salary",main="Distribution of salary amount")
#Age
hist(train_df$age,xlab="Age",main="Distribution of ages")
#Education
x<-barplot(table(train_df$elevel), xaxt="n")
text(cex=1, x=x-.25, y=-300, c('Less than HS', 'HS','Some College','4-y college','Mst/Doc/ProfD'), xpd=TRUE, srt=45)
#Car
x<-barplot(table(train_df$car), xaxt="n")
text(cex=1, x=x-.25, y=-100, c('BMW','Buick','Cadillac','Chevrolet','Chrysler','Dodge','Ford','Honda','Hyundai','Jeep','Kia','Lincoln','Mazda','Mercedes','Mitsubishi','Nissan','Ram','Subaru','Toyota','Other'), xpd=TRUE, srt=90)
#Zip Code
x<-barplot(table(train_df$zipcode), xaxt="n")
text(cex=1, x=x-.25, y=-200, c('New England','Mid-Atlantic','E-N-Central','W-N-Central','S-Atlantic','E-S-Central','W-S-Central','Mountain','Pacific'), xpd=TRUE, srt=60)
#Credit
hist(train_df$credit,xlab="Credit",main="Distribution of credit amount")
#Computer Brand
barplot(table(train_df$brand),names.arg = c('Acer','Sony'))
#Salary
hist(test_df$salary,xlab="Salary",main="Distribution of salary amount")
#Age
hist(test_df$age,xlab="Age",main="Distribution of ages")
#Education
x<-barplot(table(test_df$elevel), xaxt="n")
text(cex=1, x=x-.25, y=-150, c('Less than HS', 'HS','Some College','4-y college','Mst/Doc/ProfD'), xpd=TRUE, srt=45)
#Car
x<-barplot(table(test_df$car), xaxt="n")
text(cex=1, x=x-.25, y=-40, c('BMW','Buick','Cadillac','Chevrolet','Chrysler','Dodge','Ford','Honda','Hyundai','Jeep','Kia','Lincoln','Mazda','Mercedes','Mitsubishi','Nissan','Ram','Subaru','Toyota','Other'), xpd=TRUE, srt=90)
#Zip Code
x<-barplot(table(test_df$zipcode), xaxt="n")
text(cex=1, x=x-.25, y=-100, c('New England','Mid-Atlantic','E-N-Central','W-N-Central','S-Atlantic','E-S-Central','W-S-Central','Mountain','Pacific'), xpd=TRUE, srt=60)
#Credit
hist(test_df$credit,xlab="Credit",main="Distribution of credit amount")
#Computer Brand
barplot(table(test_df$brand),names.arg = c('Acer','Sony'))
plot(train_df$salary,train_df$age)
plot(train_df$salary,train_df$elevel)
plot(train_df$salary,train_df$car)
plot(train_df$salary,train_df$zipcode)
plot(train_df$salary,train_df$credit)
plot(train_df$salary,train_df$brand)
plot(train_df$age,train_df$elevel)
plot(train_df$age,train_df$car)
plot(train_df$age,train_df$zipcode)
plot(train_df$age,train_df$credit)
plot(train_df$age,train_df$brand)
plot(train_df$elevel,train_df$car)
plot(train_df$elevel,train_df$zipcode)
plot(train_df$elevel,train_df$credit)
plot(train_df$elevel,train_df$brand)
plot(train_df$car,train_df$zipcode)
plot(train_df$car,train_df$credit)
plot(train_df$car,train_df$brand)
plot(train_df$zipcode,train_df$credit)
plot(train_df$zipcode,train_df$brand)
plot(train_df$credit,train_df$brand)
qqnorm(train_df$salary)
qqnorm(train_df$age)
qqnorm(as.integer(train_df$elevel))
qqnorm(as.integer(train_df$car))
qqnorm(as.integer(train_df$zipcode))
qqnorm(train_df$credit)
qqnorm(as.integer(train_df$brand))
qqnorm(test_df$salary)
qqnorm(test_df$age)
qqnorm(as.integer(test_df$elevel))
qqnorm(as.integer(test_df$car))
qqnorm(as.integer(test_df$zipcode))
qqnorm(test_df$credit)
qqnorm(as.integer(test_df$brand))
ggplot(train_df, aes(x=as.factor(brand), y=salary)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=age)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(elevel))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(car))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=as.integer(zipcode))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(train_df, aes(x=as.factor(brand), y=credit)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=salary)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=age)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(elevel))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(car))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=as.integer(zipcode))) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
ggplot(test_df, aes(x=as.factor(brand), y=credit)) +
geom_boxplot(fill="slateblue", alpha=0.2) +
xlab("brand")
OneHot_train_df <- data.frame(model.matrix( ~ ., data=train_df))
colnames(OneHot_train_df)[colnames(OneHot_train_df) == 'brand1'] <- 'brand'
OneHot_train_df$brand <- as.factor(OneHot_train_df$brand) #Reconvert brand into binary column not integer.
OneHot_train_df
set.seed(107)
inTrain <- createDataPartition(y = OneHot_train_df$brand,
## the outcome data are needed
p = .75,# The percentage of data in the training set should be 75%
list = FALSE)
#The output is a set of integers for the rows of Sonar that belong in the training set.
training <- OneHot_train_df[ inTrain,] #TrainSet will contain the selected indices.
testing <- OneHot_train_df[-inTrain,] #TestSet will contain only the remaining rows that have not been selected.
nrow(training)
nrow(testing)
library(gbm)
library(Rcpp)
gbmfitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1)
# tuneLenght = 1 (trains with 1 mtry value for RandomForest)
gbmFit1 <- train(brand~., #y, target
data = training, #X, features
metric = 'Kappa', #Metric applied
method = "gbm", #ML algorithm
trControl=gbmfitControl, #Apply CV to the training
tuneLength = 10, # Number of levels for each tuning parameters that should be generated
verbose = FALSE)
gbmFit1
varImp(gbmFit1)
pred_GBM <- predict(gbmFit1, newdata = testing)
Prob_pred_GBM <- predict(gbmFit1, newdata = testing, type = "prob")
postResample(pred_GBM, testing$brand)
confusionMatrix(data = pred_GBM,
reference = testing$brand,
positive = "1")
